# VisionForge üî≠üß†

<p >
  <img src="https://github.com/user-attachments/assets/99ad5c43-fd55-429b-9e6e-dc27994a39f0" alt="VisionForge Logo" width="400">
  <br>
  <em>Forge powerful vision-language models for specialized domains</em>
</p>

[![Python 3.9+](https://img.shields.io/badge/python-3.9+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-red.svg)](https://pytorch.org/)
[![Hugging Face](https://img.shields.io/badge/ü§ó-Transformers-yellow.svg)](https://huggingface.co/docs/transformers/index)

## üåü Overview

VisionForge is a streamlined, production-ready toolkit for fine-tuning vision-language models (VLMs) on domain-specific image datasets. Transform general-purpose models into specialized experts that understand your visual domain with impressive accuracy and minimal computational resources.

**Perfect for:** healthcare professionals, scientific researchers, industrial inspectors, specialized content creators, and any domain expert seeking to harness AI vision capabilities for their specific field.

## ‚ú® Key Features

- **Memory-Efficient Fine-tuning**: Train 11B+ parameter models on a single consumer GPU using 4-bit quantization
- **Comprehensive Adaptation**: Fine-tune vision, language, and cross-attention components simultaneously
- **Domain Specialization**: Adapt pre-trained models to specialized domains (medical imaging, industrial inspection, etc.)
- **One-Command Deployment**: Push fine-tuned models directly to Hugging Face Hub
- **Built-in Evaluation**: Compare before/after performance with integrated evaluation tools
- **Flexible Configuration**: Customize all aspects of training via command line or config files
- **Hardware-Aware Optimizations**: Automatic detection of hardware capabilities (BF16 support, etc.)

## üìã Requirements

- Python 3.9+
- CUDA-compatible GPU with 16GB+ VRAM (24GB+ recommended for larger models)
- CUDA 11.7+ and cuDNN 8.5+

## üöÄ Installation

```bash
# Clone repository
git clone https://github.com/yourusername/visionforge.git
cd visionforge

# Create and activate virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
```

## üèÅ Quickstart

```bash
# Fine-tune a model on a sample medical dataset
python visionforge.py --model_name "unsloth/Llama-3.2-11B-Vision-Instruct" \
                     --dataset_name "unsloth/Radiology_mini" \
                     --output_dir "output/my_radiology_model" \
                     --max_steps 100
```

## üéØ Use Cases

### Medical Imaging Analysis
```bash
python visionforge.py --model_name "unsloth/Llama-3.2-11B-Vision-Instruct" \
                     --dataset_name "your-medical-dataset" \
                     --hub_model_id "your-username/medical-llm-expert" \
                     --instruction "You are a medical imaging expert. Describe what you see in this image in professional medical terminology."
```

### Industrial Inspection
```bash
python visionforge.py --model_name "unsloth/Llama-3.2-11B-Vision-Instruct" \
                     --dataset_name "your-industrial-dataset" \
                     --lora_rank 32 \
                     --batch_size 4 \
                     --instruction "Identify any defects or anomalies in this industrial component."
```

## ‚öôÔ∏è Advanced Configuration

VisionForge offers extensive customization options:

```bash
python visionforge.py --help
```

Key parameters:
- `--model_name`: Base model to fine-tune
- `--dataset_name`: Dataset name on HuggingFace Hub or local path
- `--lora_rank`: Rank for LoRA adaptation (higher = more capacity)
- `--batch_size`: Training batch size
- `--learning_rate`: Learning rate for training
- `--max_steps`: Maximum number of training steps
- `--hub_model_id`: Model ID for HuggingFace Hub (username/model_name)

## üìä Performance

| Model Size | GPU VRAM Required | Training Time (1000 examples) | Speedup vs. Full Fine-tuning |
|------------|-------------------|-------------------------------|------------------------------|
| 7B         | ~14GB             | ~2 hours                      | ~10x                         |
| 11B        | ~20GB             | ~3 hours                      | ~12x                         |
| 13B        | ~24GB             | ~4 hours                      | ~15x                         |
| 70B        | ~40GB             | ~8 hours                      | ~25x                         |

## üìö Dataset Format

Your dataset should contain image-caption pairs in the following format:

```python
{
  "image": PIL.Image.Image,  # Image data or path
  "caption": "Detailed description of the image"
}
```

See the example datasets on HuggingFace Hub for reference:
- [unsloth/Radiology_mini](https://huggingface.co/datasets/unsloth/Radiology_mini)

## üìã To-Do List

- [ ] Add support for RLHF fine-tuning
- [ ] Implement model merging for ensemble capabilities
- [ ] Add multi-GPU training support
- [ ] Create web UI for interactive fine-tuning
- [ ] Add more specialized templates for different domains

## ü§ù Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

1. Fork the repository
2. Create your feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add some amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## üìú License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## üôè Acknowledgements

- [Unsloth](https://github.com/unslothai/unsloth) for their optimized implementation of vision-language models
- [Hugging Face](https://huggingface.co/) for their transformers library and model hosting
- [TRL](https://github.com/huggingface/trl) for their SFT implementation

---

<p align="center">
  Made with ‚ù§Ô∏è for the AI vision community
</p>
